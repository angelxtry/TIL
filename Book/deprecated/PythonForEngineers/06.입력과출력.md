# 입력과 입출력

## 파일 읽기
* read: 파일 전체를 읽어들인다.
* read([size]): size 만큼 읽어들인다.
* readline: 한 줄씩 읽어들인다.
* readlines: 모든 줄을 읽어들인 후 한 줄씩 처리

## 파일 쓰기
* write: 줄바꿈을 위해 `\n`을 추가해야 한다.
* writelines: 자동으로 줄바꿈이 된다.

## 데이터 파일 입출력하기
### BOM 처리하기 & csv 표준 라이브러리 모듈
```py
import csv


with open('data1_utf-8.csv', 'r', encoding='utf-8') as f:
    dat = [k for k in csv.reader(f)]

print(dat)
```

```
[['\ufeffBOND_ID', 'KOR_BOND_NAME', 'ISSUE_DAY', 'MAT_DAY'], ...
```
* windows에서 UTF-8 encoding으로 csv 파일을 만들고 open()과 csv.reader() 메소드를 이용하여 내용을 읽고 출력했다.
* 출력결과에 `\ufeff`가  보인다. 이것은 BOM(Byte Order Mark)다.
* UTF-8로 파일을 작성할 때 BOM이 있으면 문제가 될 소지가 있다.
* 하자만 파일에 한글이 포함되어 있는 경우 BOM이 없으면 한글이 깨지는 편집기가 종종 있다.

```py
import csv


with open('data1_utf-8.csv', 'r', encoding='utf-8-sig') as f:
    dat = [k for k in csv.reader(f)]

print(dat)
```
* replace('\ufeff', '')와 같은 방식으로 BOM을 처리할 수 있으나 encoding을 `utf-8-sig`로 처리하면 파일을 읽으면서 BOM을 제거한다.

```py
import csv


with open('data1_utf-8.csv', 'r', encoding='utf-8') as f:
    dat = [k for k in csv.reader(f)]

print(dat)

with open('out_with_bom.csv', 'w', newline='') as f:
    writer = csv.writer(f)
    writer.writerows(dat)
```
* `utf-8`로 encoding하여 BOM이 존재하는 경우 windows에서 encoding 없이 파일 write를 하려고 하면 다음과 같은 예외가 발생한다.

```
Traceback (most recent call last):
  File "csv_process.py", line 15, in <module>
    writer.writerows(dat)
UnicodeEncodeError: 'cp949' codec can't encode character '\ufeff' in position 0: illegal multibyte sequence
```

```py
import csv


with open('data1_utf-8.csv', 'r', encoding='utf-8') as f:
    dat = [k for k in csv.reader(f)]


with open('out_with_bom.csv', 'w', newline='', encoding='utf-8') as f:
    writer = csv.writer(f)
    writer.writerows(dat)


with open('out_with_bom.csv', 'r', encoding='utf-8') as f:
    dat = [k for k in csv.reader(f)]

print(dat)
```
* BOM이 존재하는 경우 write를 하위기해 open할 때도 encoding을 명시해야 한다.
* BOM은 그대로 `out_with_bom.csv`에 기록된다.

```py
import csv


with open('data1_utf-8.csv', 'r', encoding='utf-8-sig') as f:
    dat = [k for k in csv.reader(f)]

with open('out_without_bom.csv', 'w', newline='') as f:
    writer = csv.writer(f)
    writer.writerows(dat)

with open('out_without_bom.csv', 'r') as f:
    dat = [k for k in csv.reader(f)]

print(dat)
```
* BOM을 제거하면 write에 encoding 없이도 잘 처리된다.

### pickle 파일 입출력
```py
import pickle

mydata = [1, 2, 3]

with open('data1.pickle', 'wb') as f:
    pickle.dump(mydata, f)

with open('data1.pickle', 'rb') as f:
    dat = pickle.load(f)

print(dat)
```
* dump로 저장하고 load로 데이터를 읽어들인다.
* 항상 바이너리 모드로 처리한다.

```py
import pickle

import numpy as np


a = np.float(2.3)
b = np.array([[1.1, 2.2, 3.3], [4.4, 5.5, 6.6]])
c = {'abc': 1, 'def': 2, 'ghi': 3}

with open('data1.pickle', 'wb') as f:
    pickle.dump(a, f)
    pickle.dump(b, f)
    pickle.dump(c, f)

with open('data1.pickle', 'rb') as f:
    a1 = pickle.load(f)
    b1 = pickle.load(f)
    c1 = pickle.load(f)

print(a1, b1, c1)
```
* dump를 여러 번 사용하여 하나 이상의 객체를 pickle로 만든다.
* 읽어들일 때도 load를 반복하여 사용한다.
* obhect가 몇 개나 dump되어 있는지 알 수 없다면 try문에서 예외가 발생할 때까지 시도한다.
* `EOFError` 예외가 발생한다.

```py
import pickle

import numpy as np


a = np.float(2.3)
b = np.array([[1.1, 2.2, 3.3], [4.4, 5.5, 6.6]])
c = {'abc': 1, 'def': 2, 'ghi': 3}

with open('data1.pickle', 'wb') as f:
    pickle.dump([a, b, c], f)

with open('data1.pickle', 'rb') as f:
    [a1, b1, c1] = pickle.load(f)

print(a1, b1, c1)
```
* 각각 한 라인에서 dump와 load를 처리할 수 있다.

```py
import pickle

import numpy as np


def pickle_vars(fname, **kwargs):
    with open(fname, 'wb') as f:
        pickle.dump(kwargs, f)


if __name__ == '__main__':
    a = np.float(2.3)
    b = np.array([[1.1, 2.2, 3.3], [4.4, 5.5, 6.6]])
    c = {'abc': 1, 'def': 2, 'ghi': 3}

    pickle_vars('dic1.pickle', a=a, b=b, c=c)

    with open('dic1.pickle', 'rb') as f:
        dat = pickle.load(f)
        print(dat)
```
* dictionary 타입을 활용하면 load할 때 새로운 변수를 지정하지 않아도 된다.

## pandas의 데이터 입출력 하기
* 데이터를 읽을 때 사용하는 read_csv 등을 pandas에 정의된 함수다.
* 데이터를 출력하는 데 사용하는 to_csv 같은 것은 pandas에 정의된 객체의 메서드다.

* 텍스트 파일(구분자): read_csv, to_csv
* 텍스트 파일(구분자가 `\t`): read_table, to_csv
* 텍스트 파일(컬럼폭이 고정): read_fwf, to_csv
* json: read_json, to_json
* html
* excel
* hdf5
* sql
* pickle

```py
In [1]: import pandas as pd

In [2]: import numpy as np

In [3]: df  = pd.DataFrame(np.random.randn(1000000, 2), columns=list('AB'))

In [4]: df.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 1000000 entries, 0 to 999999
Data columns (total 2 columns):
A    1000000 non-null float64
B    1000000 non-null float64
dtypes: float64(2)
memory usage: 15.3 MB

In [5]: %timeit df.to_csv('pandas_sample.csv')
1 loop, best of 3: 4.89 s per loop
```
* 백만개의 난수 배열 2개를 생성
* 첫 컬럼은 index, 다음 컬럼은 각각 A, B
* 메모리 사용은 15.3MB
* CSV 파일은 약 44.9MB
* CSV 파일로 만드는데 약 4.89초가 걸렸다.
* excel 파일로 만드는 것보다는 빠르고, HDF5로 만드는 것보다는 느리다.

### 텍스트 데이터 입출력
```py
In [2]: import pandas as pd

In [10]: dat = pd.read_csv('data1_utf-8.csv',
    ...: encoding='utf-8',
    ...: header=0,
    ...: sep=',')

In [11]: dat.head()
Out[11]:
        BOND_ID KOR_BOND_NAME  ISSUE_DAY     MAT_DAY
0  KR6290891821    티월드제21차1-1   20180212         NaN
1  KR6278064821    네오카본제칠차2-1   20180212  20230620.0

In [12]: dat.dtypes
Out[12]:
BOND_ID           object
KOR_BOND_NAME     object
ISSUE_DAY          int64
MAT_DAY          float64
dtype: object
```
* header를 지정하면 header이전의 row는 무시된다.
* `skiprow=1` 과 같이 생략할 row를 지정할 수 있다.
* sep=','는 구분자를 지정한 것이다. default가 `,`이므로 생략할 수 있다.
* sep 대신 delimiter도 동일하다.
* 데이터가 빠진 곳은 `NaN`으로 출력된다.
* pandas는 읽어들인 데이터의 타입을 자동으로 추정하여 결정한다.
* 문자열은 object 타입으로 되어 있다.

## 일부 데이터만 로드하기
```py
data = pd.read_csv('sample.csv', nrows=5)
```
* 파일의 첫 5라인만 읽어온다.
