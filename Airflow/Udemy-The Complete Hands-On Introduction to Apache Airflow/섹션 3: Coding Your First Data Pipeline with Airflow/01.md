# 섹션 3: Coding Your First Data Pipeline with Airflow

## introduction

question

1. data pipeline을 어떻게 만드는가?
2. data pipeline과 third party tool은 어떻게 interaction 하는가?
3. 이전 task에서 생성한 file을 어떻게 확인할까?

## 14. What is a DAG?

Node == Task

Edge == Dependency

## 16. [Practice] DAG Skeleton

```py
from airflow.models import DAG
from airflow.operators.mssql_operator import MySqlOperator
from datetime import datetime

default_args = {
    'start_date': datetime(2021, 1, 1),
}

with DAG('user_processing', schedule_interval='@daily',
         default_args=default_args,
         catchup=False) as dag:
    # define tasks/operators
```

## 17. What is an Operator?

하나의 operator에 하나의 task만 연결하는 것이 좋다.

operator type
action operator: 액션을 실행
transfer operator: 데이터를 전송
sensors: 조건이 확인될 때까지 기다림

## 18. [Practice] Creating Table

mysql 연결

<https://medium.com/@prajwalnadagouda/airflow-with-mysql-and-mail-smtp-configuration-360910edaf73> 참고

web 접속 -> Admin -> Connections -> `+` 버튼 클릭

```py
Conn Id: local_mysql_default

Conn Type: MySQL

Description: 로컬 mysql connection

Host: localhost

Schema(database name): vn_csv

Login: gomicorp

Password:

Port: 3306
```

user_processing.py


```py
from airflow.models import DAG
from airflow.operators.mysql_operator import MySqlOperator
from datetime import datetime

default_args = {
    'start_date': datetime(2021, 1, 1),
}

with DAG('user_processing', schedule_interval='@daily',
         default_args=default_args,
         catchup=False) as dag:
    # define tasks/operators
    create_table = MySqlOperator(
        task_id='creating_table',
        mysql_conn_id='local_mysql_default',
        sql='''
            CREATE TABLE users (
                email VARCHAR(255) NOT NULL,
                firstname TEXT NOT NULL,
                lastname TEXT NOT NULL,
                country TEXT NOT NULL,
                username TEXT NOT NULL,
                password TEXT NOT NULL,
                PRIMARY KEY (email)
            );
        ''',
    )
```

`MySqlOperator`의 mysql_conn_id는 web에서 설정한 `Conn Id`다.

airflow webserver, airflow scheduler를 실행하고 web의 DAGs에 접속하면 user_processing이 보인다.

다음 2개의 패키지가 설치되었다.

pip install apache-airflow-providers-mysql
pip install airflow[mysql]

apache-airflow-providers-mysql==1.0.2
mysql-connector-python==8.0.18

### 테스트를 해보자.

airflow tasks test user_processing creating_table 2021-01-01

```js
> airflow tasks test user_processing creating_table 2021-01-01
[2021-03-08 14:37:18,186] {dagbag.py:440} INFO - Filling up the DagBag from /Users/gomidev/Project/Gomi/GomiFlow/workspace/airflow_home/dags
[2021-03-08 14:37:18,561] {taskinstance.py:826} INFO - Dependencies all met for <TaskInstance: user_processing.creating_table 2021-01-01T00:00:00+00:00 [None]>
[2021-03-08 14:37:18,566] {taskinstance.py:826} INFO - Dependencies all met for <TaskInstance: user_processing.creating_table 2021-01-01T00:00:00+00:00 [None]>
[2021-03-08 14:37:18,567] {taskinstance.py:1017} INFO -
--------------------------------------------------------------------------------
[2021-03-08 14:37:18,567] {taskinstance.py:1018} INFO - Starting attempt 1 of 1
[2021-03-08 14:37:18,567] {taskinstance.py:1019} INFO -
--------------------------------------------------------------------------------
[2021-03-08 14:37:18,570] {taskinstance.py:1038} INFO - Executing <Task(MySqlOperator): creating_table> on 2021-01-01T00:00:00+00:00
[2021-03-08 14:37:18,686] {taskinstance.py:1230} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=user_processing
AIRFLOW_CTX_TASK_ID=creating_table
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
[2021-03-08 14:37:18,686] {mysql.py:72} INFO - Executing:
            CREATE TABLE users (
                email VARCHAR(255) NOT NULL,
                firstname TEXT NOT NULL,
                lastname TEXT NOT NULL,
                country TEXT NOT NULL,
                username TEXT NOT NULL,
                password TEXT NOT NULL,
                PRIMARY KEY (email)
            );

[2021-03-08 14:37:18,740] {base.py:65} INFO - Using connection to: id: local_mysql_default. Host: localhost, Port: 3306, Schema: vn_csv, Login: gomicorp, Password: XXXXXXXX, extra: None
[2021-03-08 14:37:18,741] {dbapi.py:180} INFO - Running statement:
            CREATE TABLE users (
                email VARCHAR(255) NOT NULL,
                firstname TEXT NOT NULL,
                lastname TEXT NOT NULL,
                country TEXT NOT NULL,
                username TEXT NOT NULL,
                password TEXT NOT NULL,
                PRIMARY KEY (email)
            );
        , parameters: None
[2021-03-08 14:37:18,795] {dbapi.py:186} INFO - Rows affected: 0
[2021-03-08 14:37:18,799] {taskinstance.py:1135} INFO - Marking task as SUCCESS. dag_id=user_processing, task_id=creating_table, execution_date=20210101T000000, start_date=20210308T053718, end_date=20210308T053718
```

실제로 테이블도 만들어진다.

```js
mysql> use vn_csv;
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A

Database changed
mysql> show tables;
+----------------------------+
| Tables_in_vn_csv           |
+----------------------------+
| calendar_table             |
| csv_delivery               |
| csv_first_inventory        |
| csv_import                 |
| csv_stock_movement_summary |
| ints                       |
| product_collections        |
| product_items              |
| stock_quantity_history     |
| users                      |
| vn_stock_count             |
+----------------------------+
11 rows in set (0.01 sec)

mysql> desc users;
+-----------+--------------+------+-----+---------+-------+
| Field     | Type         | Null | Key | Default | Extra |
+-----------+--------------+------+-----+---------+-------+
| email     | varchar(255) | NO   | PRI | NULL    |       |
| firstname | text         | NO   |     | NULL    |       |
| lastname  | text         | NO   |     | NULL    |       |
| country   | text         | NO   |     | NULL    |       |
| username  | text         | NO   |     | NULL    |       |
| password  | text         | NO   |     | NULL    |       |
+-----------+--------------+------+-----+---------+-------+
6 rows in set (0.00 sec)
```

### api 추가

테이블 생성 task를 완성했고 api check 로직을 추가하자.

web -> Admin -> Connections -> `+` 버튼 클릭

```py
Conn Id: user_api

Conn Type: HTTP

Host: https://randomuser.me/
```

code에 operation을 추가한다.

```py
from airflow.models import DAG
from airflow.operators.mysql_operator import MySqlOperator
from airflow.providers.http.sensors.http import HttpSensor
from datetime import datetime

default_args = {
    'start_date': datetime(2021, 1, 1),
}

with DAG('user_processing', schedule_interval='@daily',
         default_args=default_args,
         catchup=False) as dag:
    # define tasks/operators
    create_table = MySqlOperator(
        task_id='creating_table',
        mysql_conn_id='local_mysql_default',
        sql='''
            CREATE TABLE users (
                email VARCHAR(255) NOT NULL,
                firstname TEXT NOT NULL,
                lastname TEXT NOT NULL,
                country TEXT NOT NULL,
                username TEXT NOT NULL,
                password TEXT NOT NULL,
                PRIMARY KEY (email)
            );
        ''',
    )

    is_api_available = HttpSensor(
        task_id='is_api_available',
        http_conn_id='user_api',
        endpoint='api/'
    )
```

airflow tasks test user_processing is_api_available 2021-01-01

위와 같이 테스트를 실행할 수 있다.


```js
> airflow tasks test user_processing is_api_available 2021-01-01
[2021-03-08 14:54:16,868] {dagbag.py:440} INFO - Filling up the DagBag from /Users/gomidev/Project/Gomi/GomiFlow/workspace/airflow_home/dags
[2021-03-08 14:54:17,225] {taskinstance.py:826} INFO - Dependencies all met for <TaskInstance: user_processing.is_api_available 2021-01-01T00:00:00+00:00 [None]>
[2021-03-08 14:54:17,230] {taskinstance.py:826} INFO - Dependencies all met for <TaskInstance: user_processing.is_api_available 2021-01-01T00:00:00+00:00 [None]>
[2021-03-08 14:54:17,231] {taskinstance.py:1017} INFO -
--------------------------------------------------------------------------------
[2021-03-08 14:54:17,231] {taskinstance.py:1018} INFO - Starting attempt 1 of 1
[2021-03-08 14:54:17,231] {taskinstance.py:1019} INFO -
--------------------------------------------------------------------------------
[2021-03-08 14:54:17,234] {taskinstance.py:1038} INFO - Executing <Task(HttpSensor): is_api_available> on 2021-01-01T00:00:00+00:00
[2021-03-08 14:54:17,323] {taskinstance.py:1230} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=user_processing
AIRFLOW_CTX_TASK_ID=is_api_available
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
[2021-03-08 14:54:17,323] {http.py:99} INFO - Poking: api/
[2021-03-08 14:54:17,327] {base.py:65} INFO - Using connection to: id: user_api. Host: https://randomuser.me/, Port: None, Schema: , Login: , Password: None, extra: None
[2021-03-08 14:54:17,327] {http.py:140} INFO - Sending 'GET' to url: https://randomuser.me/api/
[2021-03-08 14:54:17,943] {base.py:245} INFO - Success criteria met. Exiting.
[2021-03-08 14:54:17,959] {taskinstance.py:1135} INFO - Marking task as SUCCESS. dag_id=user_processing, task_id=is_api_available, execution_date=20210101T000000, start_date=20210308T055417, end_date=20210308T055417
```

## extracing user

code

```py
from airflow.models import DAG
from airflow.operators.mysql_operator import MySqlOperator
from airflow.providers.http.sensors.http import HttpSensor
from airflow.providers.http.operators.http import SimpleHttpOperator
from datetime import datetime
import json

default_args = {
    'start_date': datetime(2021, 1, 1),
}

with DAG('user_processing', schedule_interval='@daily',
         default_args=default_args,
         catchup=False) as dag:
    # define tasks/operators
    create_table = MySqlOperator(
        task_id='creating_table',
        mysql_conn_id='local_mysql_default',
        sql='''
            CREATE TABLE users (
                email VARCHAR(255) NOT NULL,
                firstname TEXT NOT NULL,
                lastname TEXT NOT NULL,
                country TEXT NOT NULL,
                username TEXT NOT NULL,
                password TEXT NOT NULL,
                PRIMARY KEY (email)
            );
        ''',
    )

    is_api_available = HttpSensor(
        task_id='is_api_available',
        http_conn_id='user_api',
        endpoint='api/'
    )

    extracting_user = SimpleHttpOperator(
        task_id='extracting_user',
        http_conn_id='user_api',
        endpoint='api/',
        method='GET',
        response_filter=lambda response: json.loads(response.text),
        log_response=True
    )
```

test

```py
airflow tasks test user_processing extracting_user 2021-01-01
```

result

```js
> airflow tasks test user_processing extracting_user 2021-01-01
[2021-03-08 16:03:36,732] {dagbag.py:440} INFO - Filling up the DagBag from /Users/gomidev/Project/Gomi/GomiFlow/workspace/airflow_home/dags
[2021-03-08 16:03:37,823] {taskinstance.py:826} INFO - Dependencies all met for <TaskInstance: user_processing.extracting_user 2021-01-01T00:00:00+00:00 [None]>
[2021-03-08 16:03:37,830] {taskinstance.py:826} INFO - Dependencies all met for <TaskInstance: user_processing.extracting_user 2021-01-01T00:00:00+00:00 [None]>
[2021-03-08 16:03:37,830] {taskinstance.py:1017} INFO -
--------------------------------------------------------------------------------
[2021-03-08 16:03:37,830] {taskinstance.py:1018} INFO - Starting attempt 1 of 1
[2021-03-08 16:03:37,830] {taskinstance.py:1019} INFO -
--------------------------------------------------------------------------------
[2021-03-08 16:03:37,833] {taskinstance.py:1038} INFO - Executing <Task(SimpleHttpOperator): extracting_user> on 2021-01-01T00:00:00+00:00
[2021-03-08 16:03:37,948] {taskinstance.py:1230} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=user_processing
AIRFLOW_CTX_TASK_ID=extracting_user
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
[2021-03-08 16:03:37,948] {http.py:106} INFO - Calling HTTP method
[2021-03-08 16:03:37,968] {base.py:65} INFO - Using connection to: id: user_api. Host: https://randomuser.me/, Port: None, Schema: , Login: , Password: None, extra: None
[2021-03-08 16:03:37,969] {http.py:140} INFO - Sending 'GET' to url: https://randomuser.me/api/
[2021-03-08 16:03:38,671] {http.py:110} INFO - {"results":[{"gender":"female","name":{"title":"Miss","first":"Ceyhan","last":"Düşenkalkar"},"location":{"street":{"number":1813,"name":"Tunalı Hilmi Cd"},"city":"Afyonkarahisar","state":"Rize","country":"Turkey","postcode":57246,"coordinates":{"latitude":"75.9224","longitude":"94.6912"},"timezone":{"offset":"+5:45","description":"Kathmandu"}},"email":"ceyhan.dusenkalkar@example.com","login":{"uuid":"341b2a0a-80c8-4d53-ac63-81d9492987ca","username":"blueleopard170","password":"gretzky","salt":"aUI1unOW","md5":"79d14aa9484d564b5a2d7090b2d8b3e4","sha1":"8445229e92be1ec5b1a425a704de3dde551e2a40","sha256":"ff377503f28336c5fe27567e0bb1d5bc6d4d7e869dacdc8ade521877e8f24743"},"dob":{"date":"1989-03-13T09:01:24.014Z","age":32},"registered":{"date":"2012-02-15T16:18:57.842Z","age":9},"phone":"(516)-547-6275","cell":"(974)-066-7058","id":{"name":"","value":null},"picture":{"large":"https://randomuser.me/api/portraits/women/24.jpg","medium":"https://randomuser.me/api/portraits/med/women/24.jpg","thumbnail":"https://randomuser.me/api/portraits/thumb/women/24.jpg"},"nat":"TR"}],"info":{"seed":"1174d61724c35df3","results":1,"page":1,"version":"1.3"}}
[2021-03-08 16:03:38,681] {taskinstance.py:1135} INFO - Marking task as SUCCESS. dag_id=user_processing, task_id=extracting_user, execution_date=20210101T000000, start_date=20210308T070337, end_date=20210308T070338
```

### processing user

code

```py
from airflow.models import DAG
from airflow.operators.mysql_operator import MySqlOperator
from airflow.providers.http.sensors.http import HttpSensor
from airflow.providers.http.operators.http import SimpleHttpOperator
from airflow.operators.python import PythonOperator, task

from datetime import datetime
from pandas import json_normalize
import json

default_args = {
    'start_date': datetime(2021, 1, 1),
}


def _processing_user(ti):
    users = ti.xcom_pull(task_ids=['extracting_user'])
    if not len(users) or 'results' not in users[0]:
        raise ValueError('User is empty')
    user = users[0]['results'][0]
    processed_user = json_normalize({
        'firstname': user['name']['first'],
        'lastname': user['name']['last'],
        'country': user['location']['country'],
        'username': user['login']['username'],
        'password': user['login']['password'],
        'email': user['email']
    })
    processed_user.to_csv('/tmp/processed_user.csv', index=None, header=False)


with DAG('user_processing', schedule_interval='@daily',
         default_args=default_args,
         catchup=False) as dag:
    # define tasks/operators
    create_table = MySqlOperator(
        task_id='creating_table',
        mysql_conn_id='local_mysql_default',
        sql='''
            CREATE TABLE users (
                email VARCHAR(255) NOT NULL,
                firstname TEXT NOT NULL,
                lastname TEXT NOT NULL,
                country TEXT NOT NULL,
                username TEXT NOT NULL,
                password TEXT NOT NULL,
                PRIMARY KEY (email)
            );
        ''',
    )

    is_api_available = HttpSensor(
        task_id='is_api_available',
        http_conn_id='user_api',
        endpoint='api/'
    )

    extracting_user = SimpleHttpOperator(
        task_id='extracting_user',
        http_conn_id='user_api',
        endpoint='api/',
        method='GET',
        response_filter=lambda response: json.loads(response.text),
        log_response=True
    )

    processing_user = PythonOperator(
        task_id='processing_user',
        python_callable=_processing_user
    )
```

추가된 코드만 따로 살펴보면

```py
processing_user = PythonOperator(
    task_id='processing_user',
    python_callable=_processing_user
)
```

PythonOperator를 실행하면서 _processing_user라는 함수를 호출한다.

```py
def _processing_user(ti):
    users = ti.xcom_pull(task_ids=['extracting_user'])
    if not len(users) or 'results' not in users[0]:
        raise ValueError('User is empty')
    user = users[0]['results'][0]
    processed_user = json_normalize({
        'firstname': user['name']['first'],
        'lastname': user['name']['last'],
        'country': user['location']['country'],
        'username': user['login']['username'],
        'password': user['login']['password'],
        'email': user['email']
    })
    processed_user.to_csv('/tmp/processed_user.csv', index=None, header=False)
```

_processing_user 함수는 task instance를 인자로 받는다.

task instance는 xcom_pull 메서드를 이용하여 extracting_user의 데이터를 받을 수 있다.

해당 데이터를 가공하여 csv 파일을 생성한다.

command

```py
airflow tasks test user_processing processing_user 2021-01-01
```

result

```js
> airflow tasks test user_processing processing_user 2021-01-01
[2021-03-08 16:41:07,330] {dagbag.py:440} INFO - Filling up the DagBag from /Users/gomidev/Project/Gomi/GomiFlow/workspace/airflow_home/dags
[2021-03-08 16:41:08,545] {taskinstance.py:826} INFO - Dependencies all met for <TaskInstance: user_processing.processing_user 2021-01-01T00:00:00+00:00 [None]>
[2021-03-08 16:41:08,568] {taskinstance.py:826} INFO - Dependencies all met for <TaskInstance: user_processing.processing_user 2021-01-01T00:00:00+00:00 [None]>
[2021-03-08 16:41:08,568] {taskinstance.py:1017} INFO -
--------------------------------------------------------------------------------
[2021-03-08 16:41:08,569] {taskinstance.py:1018} INFO - Starting attempt 1 of 1
[2021-03-08 16:41:08,569] {taskinstance.py:1019} INFO -
--------------------------------------------------------------------------------
[2021-03-08 16:41:08,572] {taskinstance.py:1038} INFO - Executing <Task(PythonOperator): processing_user> on 2021-01-01T00:00:00+00:00
[2021-03-08 16:41:08,719] {taskinstance.py:1230} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=user_processing
AIRFLOW_CTX_TASK_ID=processing_user
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
[2021-03-08 16:41:08,743] {python.py:118} INFO - Done. Returned value was: None
[2021-03-08 16:41:08,764] {taskinstance.py:1135} INFO - Marking task as SUCCESS. dag_id=user_processing, task_id=processing_user, execution_date=20210101T000000, start_date=20210308T074108, end_date=20210308T074108
```

### 23. [Practice] Storing users

```py
from airflow.models import DAG
from airflow.operators.mysql_operator import MySqlOperator
from airflow.providers.http.sensors.http import HttpSensor
from airflow.providers.http.operators.http import SimpleHttpOperator
from airflow.operators.python import PythonOperator

from datetime import datetime
from sqlalchemy import create_engine
import json
import os
import pandas as pd

default_args = {
    'start_date': datetime(2021, 1, 1),
}


def _processing_user(ti):
    users = ti.xcom_pull(task_ids=['extracting_user'])
    if not len(users) or 'results' not in users[0]:
        raise ValueError('User is empty')
    user = users[0]['results'][0]
    processed_user = pd.json_normalize({
        'firstname': user['name']['first'],
        'lastname': user['name']['last'],
        'country': user['location']['country'],
        'username': user['login']['username'],
        'password': user['login']['password'],
        'email': user['email']
    })
    processed_user.to_csv('/tmp/processed_user.csv', index=None, header=False)


def _csvToSql():
    try:
        engine = create_engine("mysql+pymysql://{user}:{password}@{host}/{database}"
                               .format(user=os.environ["external_raw_data_mysql_user"],
                                       password=os.environ["external_raw_data_mysql_password"],
                                       host=os.environ["external_raw_data_mysql_host"],
                                       database="vn_csv"),
                               encoding='utf-8')
        conn = engine.connect()
    except:
        print('Can\'t connect.')

    df = pd.read_csv('/tmp/processed_user.csv',
                     names=['firstname', 'lastname',
                            'country', 'username', 'password', 'email'])
    print(df.head())
    df.to_sql(name='users', con=engine, if_exists='append', index=False)

    conn.close()

    return 'Read .csv and written to the MySQL database'


with DAG('user_processing', schedule_interval='@daily',
         default_args=default_args,
         catchup=False) as dag:
    # define tasks/operators
    create_table = MySqlOperator(
        task_id='creating_table',
        mysql_conn_id='local_mysql_default',
        sql='''
            CREATE TABLE IF NOT EXISTS users (
                email VARCHAR(255) NOT NULL,
                firstname TEXT NOT NULL,
                lastname TEXT NOT NULL,
                country TEXT NOT NULL,
                username TEXT NOT NULL,
                password TEXT NOT NULL,
                PRIMARY KEY (email)
            );
        ''',
    )

    is_api_available = HttpSensor(
        task_id='is_api_available',
        http_conn_id='user_api',
        endpoint='api/'
    )

    extracting_user = SimpleHttpOperator(
        task_id='extracting_user',
        http_conn_id='user_api',
        endpoint='api/',
        method='GET',
        response_filter=lambda response: json.loads(response.text),
        log_response=True
    )

    processing_user = PythonOperator(
        task_id='processing_user',
        python_callable=_processing_user
    )

    storing_user = PythonOperator(
        task_id='storing_user',
        python_callable=_csvToSql
    )

create_table >> is_api_available >> extracting_user >> processing_user >> storing_user
```
